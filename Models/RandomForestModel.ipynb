{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1f634a8",
   "metadata": {},
   "source": [
    "# Import data, raw data analysis, random forest, boosting, hyperparameter tuning, Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a516e7-cd26-4051-a16e-93dd660854c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shapefile as shp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "!pip install imblearn\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, r2_score, mean_squared_error, mean_absolute_error, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import dalex as dx\n",
    "from dataprep.eda import plot, plot_correlation, create_report, plot_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db05883",
   "metadata": {},
   "source": [
    "# Import data \n",
    " Drop columns \n",
    " Drop NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc2f4c6-41f6-4f37-9043-e80478f5444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../Data/Frog_Orccurence_Merged.csv') # Read in the data\n",
    "# Drop the columns with low variance  \n",
    "df = data.drop(['Unnamed: 0','swe','crs','ppt','srad','def','pet','tmax','tmin','coord','min_lon','max_lon','min_lat','max_lat','ppt_station_influence','tmax_station_influence','tmin_station_influence','vap_station_influence','samples_count'], axis=1)\n",
    "df.dropna(inplace=True) # Drop the rows with missing values\n",
    "data = df # Rename the dataframe\n",
    "data['frog_c'] = data['frog_count'] # Rename the column\n",
    "del data['frog_count'] # Delete the old column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699b6234",
   "metadata": {},
   "source": [
    "## Report over raw data, including pearsons R \n",
    "\n",
    " Takes about 1 minute to run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeda1b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_report(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab745ed",
   "metadata": {},
   "source": [
    "### Labelencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb83ef72-23cc-4235-affd-f811566bb484",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['frog_c'] = pd.qcut(data.frog_c, q=[0, .1,.2,.3, .4,.5, .6,.7, .8,.9, 1], duplicates='drop') # Create a new column with the quantiles\n",
    "data['frog_c'].value_counts() # Check the distribution of the new column\n",
    "\n",
    "data['frog_count'] = LabelEncoder().fit_transform(data['frog_c']) # Encode the new column\n",
    "data # Check the new dataframe\n",
    "del data['frog_c'] # Delete the old column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610b4142",
   "metadata": {},
   "source": [
    "### Outlier capping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e73c3b2-8def-4983-bc47-b8fd4df90e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers capping for treating outliers\n",
    "def outlier_capping(x):\n",
    "    x = x.clip(upper = x.quantile(0.99)) # Cap the outliers\n",
    "    x = x.clip(lower = x.quantile(0.01)) # Cap the outliers\n",
    "    return x # Return the capped values\n",
    "\n",
    "data.iloc[:,0:-7]=data.iloc[:,0:-7].apply(lambda x: outlier_capping(x)) # Frog count is not capped as it is the target variable\n",
    "sns.stripplot(data['frog_count']) # Check the distribution of the new column "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b253e53",
   "metadata": {},
   "source": [
    "### Define the target variables and features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcb099e-dc10-4aa9-b5ff-7bd77d6e4ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "y= data['frog_count'] # Define the target variable\n",
    "x = data.drop(['frog_count'], axis=1) # Define the features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0046e6",
   "metadata": {},
   "source": [
    "### Adasyn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0e289e-5568-4b1c-a2d3-4159a47f1f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = ADASYN() # Define the resampling method\n",
    "# fit predictor and target varialbe\n",
    "x_rus, y_rus = rus.fit_resample(x, y) # Resample the data\n",
    "print('original dataset shape:', Counter(y)) # Check the distribution of the target variable\n",
    "print('Resample dataset shape', Counter(y_rus)) # Check the distribution of the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7893de",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c8058d-235a-4d72-9c19-13194e9c5e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_rus, y_rus, test_size=0.25, random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01720547-3fc5-40f3-9a3d-05917dd44781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shapes of the training and test sets\n",
    "y_rus.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a031cf",
   "metadata": {},
   "source": [
    "### Minmaxscaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b2c122-e3f2-4f04-b3b1-49229a76962f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_scaler = MinMaxScaler() # Define the scaler\n",
    "X_train_mm = mm_scaler.fit_transform(x_train) # Scale the training data\n",
    "X_test_mm = mm_scaler.transform(x_test) # Scale the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3adfb0d",
   "metadata": {},
   "source": [
    "# VIF Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73473e20-3394-410e-b465-5dcab32114bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating VIF scores & dropping variables having high VIF to avoid multicollinearity\n",
    "F = pd.DataFrame(X_train_mm,columns=x.columns).drop(['vap'],axis=1) # Define the dataframe\n",
    "# VIF dataframe\n",
    "vif_data = pd.DataFrame() # Define the dataframe\n",
    "vif_data[\"feature\"] = F.columns\n",
    "# calculating VIF for each feature\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(F.values, i) for i in range(len(F.columns))] # Calculate the VIF\n",
    "\n",
    "print(vif_data) # Print the VIF dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248e6f33-8fe5-416b-9dad-3f72f8773554",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_mm=pd.DataFrame(X_train_mm,columns=x.columns).drop(['vap'],axis=1) # Define the dataframe\n",
    "X_train_mm=X_train_mm.values # Convert the dataframe to a numpy array\n",
    "X_test_mm=pd.DataFrame(X_test_mm,columns=x.columns).drop(['vap'],axis=1) # Define the dataframe\n",
    "X_test_mm=X_test_mm.values # Convert the dataframe to a numpy array\n",
    "\n",
    "data = data.drop(['vap'],axis=1) # Define the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f61200",
   "metadata": {},
   "source": [
    "# Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69fb9ba-4341-4129-a1b4-7a88da39680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=1000, oob_score=True, random_state=123456) # Define the random forest classifier\n",
    "rf.fit(X_train_mm, y_train) # Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bd6d9a-6739-4224-9dcf-f05ede341139",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = rf.predict(X_test_mm) # Predict the test data\n",
    "accuracy = accuracy_score(y_test, predicted) # Calculate the accuracy\n",
    "\n",
    "PredTrainSet1 = rf.predict(X_train_mm) #  Predict the training data\n",
    "train_R22 = r2_score(y_train, PredTrainSet1) # Calculate the R2 score\n",
    "\n",
    "validation_R22 = r2_score(y_test, predicted) # Calculate the R2 score\n",
    "accuracyT = accuracy_score(y_train, PredTrainSet1) # Calculate the accuracy\n",
    "\n",
    "MSE2= mean_squared_error(y_test, predicted) # Calculate the MSE\n",
    "RMSE2= np.sqrt(mean_squared_error(y_test, predicted)) # Calculate the RMSE\n",
    "MAE2= mean_absolute_error(y_test, predicted) # Calculate the MAE\n",
    "\n",
    "print(f'Out-of-bag score estimate: {rf.oob_score_:.3}') # Print the out-of-bag score estimate\n",
    "print(f'Test accuracy score: {accuracy:.3}') # Print the test accuracy score\n",
    "print(f'Train accuracy score: {accuracyT:.3}') # Print the train accuracy score\n",
    "print(f'MSE: {MSE2:.3}') # Print the MSE\n",
    "print(f'RMSE2: {RMSE2:.3}') # Print the RMSE\n",
    "print(f'MAE: {MAE2:.3}') # Print the MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93b93e9",
   "metadata": {},
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a312f8-bfc1-47ff-8d6b-ad4f552089ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, predicted) # Calculate the confusion matrix\n",
    "cm # Print the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd26ce7-e3fe-42c6-af05-2568090228de",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sums = cm.sum(axis=1, keepdims=True) # Calculate the row sums\n",
    "norm_conf_mx = cm / row_sums # Normalize the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175efd15-665e-453b-ac24-07bf06d944c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.fill_diagonal(norm_conf_mx, 0) # Fill the diagonal with zeros\n",
    "plt.matshow(norm_conf_mx, cmap=plt.cm.gray) # Plot the normalized confusion matrix\n",
    "plt.show() # Show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cfe433",
   "metadata": {},
   "source": [
    "### Heatmap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae18a31-122f-4403-aadb-174679ad6d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(11.7,8.27)}) # Set the figure size\n",
    "cm = pd.DataFrame(confusion_matrix(y_test, predicted), columns=[0,1,2,3,4,5,6], index=[0,1,2,3,4,5,6]) # Create the confusion matrix\n",
    "sns.heatmap(cm, annot=True) # Plot the heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aacae6",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c20c50-871f-4d79-b48b-b4b6a2033ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for model\n",
    "explainer = dx.Explainer(rf, X_train_mm, y_train, label='frog_count') # Define the explainer\n",
    "features = x.columns.to_list() # Define the features\n",
    "features # Print the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9176be20-8863-45dc-98ba-ad457ac93f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating dict to identlfy the feature names in the below visualization\n",
    "di = {} # Define the dictionary\n",
    "i = 0 # Define the counter\n",
    "for each in features: # Loop through the features\n",
    "    di[i] = each #  Assign the feature to the dictionary\n",
    "    i=i+1 # Increment the counter\n",
    "di # Print the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9764867f-ac4f-4343-b7e2-1ecbe15fe74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the variable importance chart\n",
    "explainer.model_parts().plot() # Plot the model parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77350351",
   "metadata": {},
   "source": [
    "### Permuation importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6cbfa5-6551-4749-b111-699e0956b91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "perm_importance = permutation_importance(rf, X_test_mm, y_test) # Calculate the permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32bcee5-e11d-4e03-b85a-5a12ac787554",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idx = perm_importance.importances_mean.argsort() # Sort the permutation importance\n",
    "plt.barh(predicted[sorted_idx], perm_importance.importances_mean[sorted_idx]) # Plot the bar chart\n",
    "plt.xlabel(\"Permutation Importance\") # Label the x-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5af183-4a6e-49db-beb2-18c746af9c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predicted)) # Print the classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eab17e",
   "metadata": {},
   "source": [
    "## Grid Search Cross validation\n",
    "### Hyperparamater tuning\n",
    "FYI: takes aprox 14 hours to run with 64 core 372 GB RAM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c778f506-97f1-4527-ab9a-c49bc492dec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { \n",
    "    'n_estimators': [100, 200,300,400, 700,800,900, 1000, 1200, 1500,3000],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "} # Define the parameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2324baf5-eb3b-4b31-afd4-55e9ecff9e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_rfc = GridSearchCV(estimator=rf, param_grid=param_grid, cv= 5) # Define the cross-validation\n",
    "CV_rfc.fit(X_train_mm, y_train) # Fit the model\n",
    "print(CV_rfc.best_params_) # Print the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89878635-5cd6-47a4-b55d-f21f0c2f1ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(results):\n",
    "    print(f'Best parameters are: {results.best_params_}')\n",
    "    print(\"\\n\")\n",
    "    mean_score = results.cv_results_['mean_test_score']\n",
    "    std_score = results.cv_results_['std_test_score']\n",
    "    params = results.cv_results_['params']\n",
    "    for mean,std,params in zip(mean_score,std_score,params):\n",
    "        print(f'{round(mean,3)} + or -{round(std,3)} for the {params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0fb298-d8ec-4ffb-814f-dcb7e37f1c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(CV_rfc)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8dbc17dd0b710eab1bd68a7dcef5e99b1fb0c313119666e584e933003c90a726"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
